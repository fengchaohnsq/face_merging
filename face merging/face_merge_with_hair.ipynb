{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对静态人脸图像文件进行68个特征点的标定\n",
    "import dlib  # 人脸识别的库 Dlib\n",
    "import numpy as np,numpy  # 数据处理的库 numpy\n",
    "import cv2   # 图像处理的库 OpenCv\n",
    "import sys\n",
    "\n",
    "FACE_POINTS = list(range(17, 68))\n",
    "MOUTH_POINTS = list(range(48, 61))\n",
    "RIGHT_BROW_POINTS = list(range(17, 22))\n",
    "LEFT_BROW_POINTS = list(range(22, 27))\n",
    "RIGHT_EYE_POINTS = list(range(36, 42))\n",
    "LEFT_EYE_POINTS = list(range(42, 48))\n",
    "NOSE_POINTS = list(range(27, 35))\n",
    "JAW_POINTS = list(range(0, 17))\n",
    "\n",
    "OVERLAY_POINTS = [JAW_POINTS+LEFT_BROW_POINTS+RIGHT_BROW_POINTS,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dlib 检测器和预测器\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')\n",
    "\n",
    "#获取68个面孔特征点，输入函数是图片，返回68个图片的像素点数组，数组index从0开始\n",
    "def get_landmarks(img):\n",
    "    faces = detector(img, 1)# 人脸数，第二个参数inclass为1时检测RGB图片，参数为0时检测灰度图片\n",
    "    if len(faces) == 1:\n",
    "        return np.matrix([[p.x, p.y] for p in predictor(img, faces[0]).parts()])\n",
    "    raise NoFaceOrTooManyFaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从text文本中读取特征点\n",
    "def readPoints(path) :\n",
    "    # Create an array of points.\n",
    "    points = [];\n",
    "    # Read points\n",
    "    with open(path) as file :\n",
    "        for line in file :\n",
    "            x, y = line.split()\n",
    "            points.append((int(x), int(y)))\n",
    "\n",
    "    return points\n",
    "\n",
    "# Apply affine transform calculated using srcTri and dstTri to src and\n",
    "# output an image of size.\n",
    "def applyAffineTransform(src, srcTri, dstTri, size) :\n",
    "    #print('--------')\n",
    "    #print(srcTri)\n",
    "    # Given a pair of triangles, find the affine transform.\n",
    "    warpMat = cv2.getAffineTransform( np.float32(srcTri), np.float32(dstTri) )\n",
    "    # Apply the Affine Transform just found to the src image\n",
    "    #像素使用线性插值的方式来对RGB颜色通道进行融合，flags=cv2.INTER_LINEAR\n",
    "    dst = cv2.warpAffine( src, warpMat, (size[0], size[1]), None, flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_REFLECT_101 )\n",
    "    return dst\n",
    "\n",
    "# Warps and alpha blends triangular regions from img1 and img2 to img\n",
    "# 参数t为list类型\n",
    "def morphTriangle(img1, img2, img, t1, t2, t, alpha) :\n",
    "    # Find bounding rectangle for each triangle\n",
    "    r1 = cv2.boundingRect(np.float32([t1]))\n",
    "    r2 = cv2.boundingRect(np.float32([t2]))\n",
    "    r = cv2.boundingRect(np.float32([t]))\n",
    "    #print('merge-r=',r)\n",
    "    # Offset points by left top corner of the respective rectangles\n",
    "    t1Rect = []\n",
    "    t2Rect = []\n",
    "    tRect = []\n",
    "    \n",
    "    #这里的t1 t2 t里存放的都是面部特征点的index\n",
    "    for i in range(0, 3):\n",
    "        tRect.append(((t[i][0] - r[0]),(t[i][1] - r[1])))\n",
    "        t1Rect.append(((t1[i][0] - r1[0]),(t1[i][1] - r1[1])))\n",
    "        t2Rect.append(((t2[i][0] - r2[0]),(t2[i][1] - r2[1])))\n",
    "    # Get mask by filling triangle\n",
    "    mask = np.zeros((r[3], r[2], 3), dtype = np.float32)\n",
    "    cv2.fillConvexPoly(mask, np.int32(tRect), (1.0, 1.0, 1.0), 16, 0);\n",
    "    # Apply warpImage to small rectangular patches\n",
    "    img1Rect = img1[r1[1]:r1[1] + r1[3], r1[0]:r1[0] + r1[2]]\n",
    "    img2Rect = img2[r2[1]:r2[1] + r2[3], r2[0]:r2[0] + r2[2]]\n",
    "    size = (r[2], r[3])\n",
    "    #print('merge-shape=',img1Rect.shape)\n",
    "    warpImage1 = applyAffineTransform(img1Rect, t1Rect, tRect, size)\n",
    "    warpImage2 = applyAffineTransform(img2Rect, t2Rect, tRect, size)\n",
    "    \n",
    "    # Alpha blend rectangular patches\n",
    "    imgRect = (1.0 - alpha) * warpImage1 + alpha * warpImage2\n",
    "\n",
    "    # Copy triangular region of the rectangular patch to the output image\n",
    "    img[r[1]:r[1]+r[3], r[0]:r[0]+r[2]] = img[r[1]:r[1]+r[3], r[0]:r[0]+r[2]] * ( 1 - mask ) + imgRect * mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#利用三角仿射把被融合图片的脸部轮廓、关键点变形为上面得到的脸部关键点\n",
    "#根据morph得到的特征点把模板图的脸部做变形\n",
    "def tranSrc(src_img, src_points, dst_points):\n",
    "    \n",
    "    r = cv2.boundingRect(np.float32(dst_points))\n",
    "    center = (r[0] + int(r[2] / 2), r[1] + int(r[3] / 2)) \n",
    "    srcTri=[]\n",
    "    dstTri=[]\n",
    "    #选取脸部轮廓特征点0-26做变形即可\n",
    "    for i in range(0,34):\n",
    "        srcTri.append(src_points[i])\n",
    "        srcTri.append(src_points[i+1])\n",
    "        srcTri.append((center[0],center[1]))\n",
    "        dstTri.append(dst_points[i])\n",
    "        dstTri.append(dst_points[i+1])\n",
    "        dstTri.append((center[0],center[1]))\n",
    "        #将模板图特征点和中心点组成三角形并获取矩形\n",
    "        t = [src_points[i],src_points[i+1],(center[0],center[1])]\n",
    "        r = cv2.boundingRect(np.float32(t))\n",
    "        #print('r=',r)\n",
    "        #切割出三角形的像素和坐标\n",
    "        img = src_img[r[1]:r[1] + r[3], r[0]:r[0] + r[2]]\n",
    "        size = (r[2], r[3])\n",
    "        # Given a pair of triangles, find the affine transform.\n",
    "        #print('++++++++++')\n",
    "        #print(i,'r=',r,'t=',t)\n",
    "        #print(i,'size0=',size[0],'size1=',size[1],'img-shape=',img.shape)\n",
    "        #print(i,'srcTri=',srcTri,'dstTri=',dstTri)\n",
    "        warpMat = cv2.getAffineTransform(np.float32(srcTri), np.float32(dstTri))\n",
    "        #清空list防止参数过多\n",
    "        srcTri.clear()\n",
    "        dstTri.clear()\n",
    "        #print(i,'warp=',warpMat)\n",
    "        #Apply the Affine Transform just found to the src image\n",
    "        #像素使用线性插值的方式来对RGB颜色通道进行融合，flags=cv2.INTER_LINEAR\n",
    "        #print(i,'src_img',src_img[r[1]:r[1]+r[3], r[0]:r[0]+r[2]])\n",
    "        src_img[r[1]:r[1]+r[3], r[0]:r[0]+r[2]]= cv2.warpAffine(img, warpMat,(size[0], size[1]), None, flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_REFLECT_101 )\n",
    "        #print(i,'Affine=',src_img[r[1]:r[1]+r[3], r[0]:r[0]+r[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dst_img = merge_img(src_img, dst_img, dst_matrix, dst_points, k_size, mat_multiple)\n",
    "#最后一步是将融合后的新图片脸部区域用泊松融合算法贴到模特图上。泊松融合可直接使用opencv提供的函数\n",
    "#src_img是模板图 dst_img是融合后要贴入的图 src_poing 和dst_point是融合后的图特征点的矩阵和数组类型\n",
    "#mat_multiple —— 缩放获取到的人脸心型区域大小默认0.95\n",
    "#blur_size —— 模糊核大小，用于模糊人脸融合边缘，减少融合后的违和感,默认(15,10)\n",
    "#face_mask —— 是子图的掩图\n",
    "#center —— 表示坐标，你打算在母图的哪个位置放子图\n",
    "#cv2.NORMAL_CLONE代表融合的模式\n",
    "def merge_img(src_img, dst_img, src_points, dst_points, k_size=None, mat_multiple=None):\n",
    "    \n",
    "    face_mask = np.zeros(dst_img.shape, dtype=dst_img.dtype) \n",
    "    dst_points=np.array(dst_points)\n",
    "    for group in OVERLAY_POINTS:\n",
    "        cv2.fillConvexPoly(face_mask, cv2.convexHull(np.int32(dst_points[group])), (255, 255, 255))\n",
    "    # Display Result\n",
    "    #cv2.imshow(\"Morphed Face\",np.uint8(face_mask))\n",
    "    #cv2.waitKey(0)    \n",
    "    #center报错\n",
    "    r = cv2.boundingRect(np.float32([src_points[:16]]))\n",
    "    center = (r[0] + int(r[2] / 2), r[1] + int(r[3] / 2))\n",
    "#     width, height, channels = src_img.shape\n",
    "#     center = (height//2, width//2)\n",
    "    \n",
    "    if mat_multiple:\n",
    "        mat = cv2.getRotationMatrix2D(center, 0, mat_multiple)\n",
    "        face_mask = cv2.warpAffine(face_mask, mat, (face_mask.shape[1], face_mask.shape[0]))    \n",
    "    if k_size:\n",
    "        face_mask = cv2.blur(face_mask, k_size, center)\n",
    "        #cv2.seamlessClone(src, dst, mask, center, flags)\n",
    "        #src:将要被克隆到目标图上的原图，融合后的面孔。\n",
    "        #dst:目标图，模板图。\n",
    "        #mask：想要克隆的目标图的mask。mask的大小要与原图大小一致。\n",
    "        #center ：目标影像的中心在背景图像上的坐标！注意是目标影像的中心\n",
    "        #flags： cv2.NORMAL_CLONE and cv2.MIXED_CLONE. \n",
    "        #cv2.seamlessClone返回图片 \n",
    "    return cv2.seamlessClone(dst_img, src_img, face_mask, center, cv2.NORMAL_CLONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "/feedstock_root/build_artefacts/opencv_1513957419294/work/opencv-3.3.0/modules/imgproc/src/color.cpp:10600: error: (-215) depth == CV_8U || depth == CV_16U || depth == CV_32F in function cvtColor\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-6ae949998d99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;31m#泊松融合\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mmorph_points_matrix\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmorph_points\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mdst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmerge_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmorph_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoints1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmorph_points\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.95\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;31m# Display Result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-39-8e9c18735aee>\u001b[0m in \u001b[0;36mmerge_img\u001b[0;34m(src_img, dst_img, src_points, dst_points, k_size, mat_multiple)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m#flags： cv2.NORMAL_CLONE and cv2.MIXED_CLONE.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m#cv2.seamlessClone返回图片\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseamlessClone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mface_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcenter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNORMAL_CLONE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31merror\u001b[0m: /feedstock_root/build_artefacts/opencv_1513957419294/work/opencv-3.3.0/modules/imgproc/src/color.cpp:10600: error: (-215) depth == CV_8U || depth == CV_16U || depth == CV_32F in function cvtColor\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    filename1 = '/home/fengchao/桌面/11.png'\n",
    "    filename2 = '/home/fengchao/桌面/3.png'\n",
    "    alpha = 0.5 #alpha调节融合程度，数值越小第一张图融合成分越多\n",
    "    \n",
    "    # 读取图片\n",
    "    img1 = cv2.imread(filename1);\n",
    "    img2 = cv2.imread(filename2);\n",
    "\n",
    "    # 将图片像素值转换成浮点数\n",
    "    img1 = np.float64(img1)\n",
    "    img2 = np.float64(img2)\n",
    "    # 读取两张图片的特征点\n",
    "    points1 = readPoints(filename1 + '.txt')\n",
    "    points2 = readPoints(filename2 + '.txt')\n",
    "\n",
    "    #创建一个list存放morph后的特征点\n",
    "    morph_points = []\n",
    "    #计算alpha权重特征点获取融合特征点\n",
    "    for i in range(0, len(points1)):\n",
    "        x = ( 1 - alpha ) * points1[i][0] + alpha * points2[i][0]\n",
    "        y = ( 1 - alpha ) * points1[i][1] + alpha * points2[i][1]\n",
    "        morph_points.append((x,y))\n",
    "        \n",
    "    #为  模板面孔赋值用于生成基于图片一的图片空间\n",
    "    morph_img= np.zeros(img1.shape,dtype = img1.dtype)\n",
    "    src_img= np.zeros(img1.shape,dtype = img1.dtype)\n",
    "    # 从tri.txt文档中读入面部三角\n",
    "    with open(\"tri.txt\") as file :\n",
    "        for line in file :\n",
    "            x,y,z = line.split()\n",
    "            \n",
    "            x = int(x)\n",
    "            y = int(y)\n",
    "            z = int(z)\n",
    "            \n",
    "            t1 = [points1[x], points1[y], points1[z]]\n",
    "            t2 = [points2[x], points2[y], points2[z]]\n",
    "            t = [morph_points[x], morph_points[y], morph_points[z]]\n",
    "            #三角融合算法，对上一步转换后的待融合图片再次取关键点，然后与模特图的关键点一起做三角融合成新的图片\n",
    "            morphTriangle(img1, img2, morph_img, t1, t2, t, alpha)     \n",
    "    #对模板进行三角仿射变形\n",
    "    #src_img=img1\n",
    "    #tranSrc(src_img, points1, morph_points)\n",
    "    cv2.imwrite('/home/fengchao/桌面/morph_img.jpg',morph_img)\n",
    "    #泊松融合\n",
    "    morph_points_matrix= np.mat(morph_points)\n",
    "    dst = merge_img(img1, morph_img, points1, morph_points,(15,10),0.95)\n",
    "\n",
    "    # Display Result\n",
    "    #cv2.imshow(\"Morphed Face\",np.uint8(dst_img))\n",
    "    #cv2.waitKey(0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Read images : src image will be cloned into dst\n",
    "    im = cv2.imread('/home/fengchao/桌面/11.png')\n",
    "    obj= cv2.imread('/home/fengchao/桌面/morph_img.jpg')\n",
    "\n",
    "    # Create an all white mask\n",
    "    #mask = 255 * np.ones(obj.shape, obj.dtype)\n",
    "    face_mask = np.zeros(obj.shape, dtype=obj.dtype) \n",
    "    print(face_mask.shape)\n",
    "    dst_points=np.array(morph_points)\n",
    "    for group in OVERLAY_POINTS:\n",
    "        cv2.fillConvexPoly(face_mask, cv2.convexHull(np.int32(dst_points[group])), (255, 255, 255))\n",
    "    print(face_mask.shape)\n",
    "    # The location of the center of the src in the dst\n",
    "    width, height, channels = im.shape\n",
    "    center = (height//2, width//2)\n",
    "    r = cv2.boundingRect(np.float32([points1[:16]]))\n",
    "    center1 = (r[0] + int(r[2] / 2), r[1] + int(r[3] / 2))\n",
    "    \n",
    "    print (center)\n",
    "    print (center1)\n",
    "    # Seamlessly clone src into dst and put the results in output\n",
    "    normal_clone = cv2.seamlessClone(obj, im, mask, center, cv2.NORMAL_CLONE)\n",
    "\n",
    "    # Write results\n",
    "    cv2.imwrite(\"opencv-normal-clone-example.jpg\", normal_clone)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#最后一步是将融合后的新图片脸部区域用泊松融合算法贴到模特图上。泊松融合可直接使用opencv提供的函数\n",
    "#src_img是模板图 dst_img是融合后要贴入的图 dst_matrix 和dst_point是融合后的图特征点的矩阵和数组类型\n",
    "#mat_multiple —— 缩放获取到的人脸心型区域大小默认0.95\n",
    "#blur_size —— 模糊核大小，用于模糊人脸融合边缘，减少融合后的违和感,默认(15,10)\n",
    "#face_mask —— 是子图的掩图\n",
    "#center —— 表示坐标，你打算在母图的哪个位置放子图\n",
    "#cv2.NORMAL_CLONE代表融合的模式\n",
    "def merge_img(src_img, dst_img, dst_points):\n",
    "    \n",
    "    face_mask = 255*np.ones(dst_points.shape, dtype=dst_img.dtype)    \n",
    "    cv2.imshow(\"Morphed Face\",np.uint8(face_mask))\n",
    "    cv2.waitKey(0)\n",
    "    dst_points=np.int64(dst_points)\n",
    "    #cv2.fillConvexPoly(face_mask, dst_points, (255, 255, 255))\n",
    "    #cv2.fillConvexPoly(face_mask, dst_points, (255, 255, 255))\n",
    "        \n",
    "    r = cv2.boundingRect(np.float32(dst_points))\n",
    "    center = (r[0] + int(r[2] / 2), r[1] + int(r[3] / 2)) \n",
    "        #cv2.seamlessClone(src, dst, mask, center, flags)\n",
    "        #src:将要被克隆到目标图上的原图，融合后的面孔。\n",
    "        #dst:目标图，模板图。\n",
    "        #mask：想要克隆的目标图的mask。mask的大小要与原图大小一致。\n",
    "        #center ： 在目标图中原图的中心位置。\n",
    "        #flags：  cv2.NORMAL_CLONE and cv2.MIXED_CLONE. \n",
    "        #cv2.seamlessClone返回图片\n",
    "    return cv2.seamlessClone(np.uint8(dst_img), src_img, face_mask, center, cv2.NORMAL_CLONE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
