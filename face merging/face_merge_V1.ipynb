{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对静态人脸图像文件进行68个特征点的标定\n",
    "import dlib         # 人脸识别的库 Dlib\n",
    "import numpy as np,numpy  # 数据处理的库 numpy\n",
    "import cv2   # 图像处理的库 OpenCv\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dlib 检测器和预测器\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')\n",
    "\n",
    "#获取68个面孔特征点，输入函数是图片，返回68个图片的像素点数组，数组index从0开始\n",
    "def get_landmarks(img):\n",
    "    faces = detector(img, 1)# 人脸数，第二个参数inclass为1时检测RGB图片，参数为0时检测灰度图片\n",
    "    if len(faces) == 1:\n",
    "        return np.matrix([[p.x, p.y] for p in predictor(img, faces[0]).parts()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Procrustes分析方法是对两个形状进行归一化处理 。从数学上来讲，普氏分析就是利用最小二乘法寻找形状A到形状B的仿射变换。\n",
    "#常规 Procrustes 分析法，旋转缩放移动第一个输入向量点来匹配第二输入个向量点。\n",
    "#输入参数points1和points2分别是两个图片的输入矩阵，矩阵的每一行都是面部特征点的坐标-像素。\n",
    "#输出是points1按照points2旋转后得到的矩阵。\n",
    "#特征点坐标计算列平均值，各坐标减去均值，计算全局标准差，各坐标除以标准差，进行归一化。\n",
    "\n",
    "def transformation_from_points(points1, points2):\n",
    "    \"\"\"\n",
    "    Return an affine transformation [s * R | T] such that:\n",
    "        sum ||s*R*p1,i + T - p2,i||^2\n",
    "    is minimized.\n",
    "    \"\"\"\n",
    "    # Solve the procrustes problem by subtracting centroids, scaling by the\n",
    "    # standard deviation, and then using the SVD to calculate the rotation. See\n",
    "    # the following for more details:\n",
    "    #   https://en.wikipedia.org/wiki/Orthogonal_Procrustes_problem\n",
    "    # 将特征点转换成matrix\n",
    "    points1=np.matrixlib.defmatrix.matrix(points1)\n",
    "    points2=np.matrixlib.defmatrix.matrix(points2)\n",
    "    points1 = points1.astype(numpy.float64)\n",
    "    points2 = points2.astype(numpy.float64)\n",
    "    #计算points1和points2的平均值，axis = 0：压缩行，对各列求均值，返回 1* n 矩阵，axis 不设置值，对 m*n 个数求均值，返回一个实数，axis =1 ：压缩列，对各行求均值，返回 m *1 矩阵\n",
    "    c1 = numpy.mean(points1, axis=0)\n",
    "    c2 = numpy.mean(points2, axis=0)\n",
    "    points1 -= c1\n",
    "    points2 -= c2\n",
    "    #计算全局标准差，axis=0计算每一列的标准差，axis=1计算每一行的标准差，缺省计算全局\n",
    "    s1 = numpy.std(points1)\n",
    "    s2 = numpy.std(points2)\n",
    "    points1 /= s1\n",
    "    points2 /= s2\n",
    "    #奇异值分解\n",
    "    U, S, Vt = numpy.linalg.svd(points1.T * points2)\n",
    "\n",
    "    # The R we seek is in fact the transpose of the one given by U * Vt. This\n",
    "    # is because the above formulation assumes the matrix goes on the right\n",
    "    # (with row vectors) where as our solution requires the matrix to be on the\n",
    "    # left (with column vectors).\n",
    "    R = (U * Vt).T\n",
    "\n",
    "    return numpy.vstack([numpy.hstack(((s2 / s1) * R,\n",
    "                                       c2.T - (s2 / s1) * R * c1.T)),\n",
    "                         numpy.matrix([0., 0., 1.])])\n",
    "\n",
    "#使图形根据旋转角度进行对齐操作\n",
    "def warp_im(im, M, dshape):\n",
    "    output_im = numpy.zeros(dshape, dtype=im.dtype)\n",
    "    cv2.warpAffine(im,\n",
    "                   M[:2],\n",
    "                   (dshape[1], dshape[0]),\n",
    "                   dst=output_im,\n",
    "                   borderMode=cv2.BORDER_TRANSPARENT,\n",
    "                   flags=cv2.WARP_INVERSE_MAP)\n",
    "    return output_im\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从text文本中读取特征点\n",
    "def readPoints(path) :\n",
    "    # Create an array of points.\n",
    "    points = [];\n",
    "    # Read points\n",
    "    with open(path) as file :\n",
    "        for line in file :\n",
    "            x, y = line.split()\n",
    "            points.append((int(x), int(y)))\n",
    "\n",
    "    return points\n",
    "\n",
    "\n",
    "# Apply affine transform calculated using srcTri and dstTri to src and\n",
    "# output an image of size.\n",
    "def applyAffineTransform(src, srcTri, dstTri, size) :\n",
    "    \n",
    "    # Given a pair of triangles, find the affine transform.\n",
    "    warpMat = cv2.getAffineTransform( np.float32(srcTri), np.float32(dstTri) )\n",
    "    \n",
    "    # Apply the Affine Transform just found to the src image\n",
    "    dst = cv2.warpAffine( src, warpMat, (size[0], size[1]), None, flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_REFLECT_101 )\n",
    "\n",
    "    return dst\n",
    "\n",
    "# Warps and alpha blends triangular regions from img1 and img2 to img\n",
    "def morphTriangle(img1, img2, img, t1, t2, t, alpha) :\n",
    "\n",
    "    # Find bounding rectangle for each triangle\n",
    "    r1 = cv2.boundingRect(np.float32([t1]))\n",
    "    r2 = cv2.boundingRect(np.float32([t2]))\n",
    "    r = cv2.boundingRect(np.float32([t]))\n",
    "\n",
    "\n",
    "    # Offset points by left top corner of the respective rectangles\n",
    "    t1Rect = []\n",
    "    t2Rect = []\n",
    "    tRect = []\n",
    "\n",
    "\n",
    "    for i in range(0, 3):\n",
    "        tRect.append(((t[i][0] - r[0]),(t[i][1] - r[1])))\n",
    "        t1Rect.append(((t1[i][0] - r1[0]),(t1[i][1] - r1[1])))\n",
    "        t2Rect.append(((t2[i][0] - r2[0]),(t2[i][1] - r2[1])))\n",
    "\n",
    "\n",
    "    # Get mask by filling triangle\n",
    "    mask = np.zeros((r[3], r[2], 3), dtype = np.float32)\n",
    "    cv2.fillConvexPoly(mask, np.int32(tRect), (1.0, 1.0, 1.0), 16, 0);\n",
    "\n",
    "    # Apply warpImage to small rectangular patches\n",
    "    img1Rect = img1[r1[1]:r1[1] + r1[3], r1[0]:r1[0] + r1[2]]\n",
    "    img2Rect = img2[r2[1]:r2[1] + r2[3], r2[0]:r2[0] + r2[2]]\n",
    "\n",
    "    size = (r[2], r[3])\n",
    "    warpImage1 = applyAffineTransform(img1Rect, t1Rect, tRect, size)\n",
    "    warpImage2 = applyAffineTransform(img2Rect, t2Rect, tRect, size)\n",
    "\n",
    "    # Alpha blend rectangular patches\n",
    "    imgRect = (1.0 - alpha) * warpImage1 + alpha * warpImage2\n",
    "\n",
    "    # Copy triangular region of the rectangular patch to the output image\n",
    "    img[r[1]:r[1]+r[3], r[0]:r[0]+r[2]] = img[r[1]:r[1]+r[3], r[0]:r[0]+r[2]] * ( 1 - mask ) + imgRect * mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#再次对上一步的结果图进行取点，然后运用三角仿射利将被融合图片的脸部轮廓、关键点变形为上面得到的脸部关键点\n",
    "    src_img = tran_src(src_img, src_points, dst_points, face_area)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#最后一步是将融合后的新图片脸部区域用泊松融合算法贴到模特图上。泊松融合可直接使用opencv提供的函数\n",
    "\n",
    "def merge_img(src_img, dst_img, dst_matrix, dst_points, k_size=None, mat_multiple=None):\n",
    "    face_mask = np.zeros(src_img.shape, dtype=src_img.dtype)    \n",
    "    \n",
    "    for group in core.OVERLAY_POINTS:\n",
    "        cv2.fillConvexPoly(face_mask, cv2.convexHull(dst_matrix[group]), (255, 255, 255))\n",
    "        \n",
    "    r = cv2.boundingRect(np.float32([dst_points[:core.FACE_END]]))\n",
    "\n",
    "    center = (r[0] + int(r[2] / 2), r[1] + int(r[3] / 2))    \n",
    "    \n",
    "    if mat_multiple:\n",
    "        mat = cv2.getRotationMatrix2D(center, 0, mat_multiple)\n",
    "        face_mask = cv2.warpAffine(face_mask, mat, (face_mask.shape[1], face_mask.shape[0]))    \n",
    "    \n",
    "    if k_size:\n",
    "        face_mask = cv2.blur(face_mask, k_size, center)    \n",
    "        \n",
    "    return cv2.seamlessClone(np.uint8(dst_img), src_img, face_mask, center, cv2.NORMAL_CLONE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__=='__main__':\n",
    "    \n",
    "    filename1 = '/home/fengchao/桌面/11.png'\n",
    "    filename2 = '/home/fengchao/桌面/3.png'\n",
    "    alpha = 0.5 #alpha调节融合程度，数值越小第一张图融合成分越多\n",
    "    \n",
    "    # 读取图片\n",
    "    img1 = cv2.imread(filename1);\n",
    "    img2 = cv2.imread(filename2);\n",
    "    \n",
    "    # 将图片像素值转换成浮点数\n",
    "    img1 = np.float64(img1)\n",
    "    img2 = np.float64(img2)\n",
    "\n",
    "    # 读取两张图片的特征点\n",
    "    points1 = readPoints(filename1 + '.txt')\n",
    "    points2 = readPoints(filename2 + '.txt')\n",
    "\n",
    "    #对图二进行旋转缩放移动第一张图向量点来匹配第一张图向量点。\n",
    "    M= transformation_from_points(points2, points1)\n",
    "    #使图像二根据图一脸部进行图片整体旋转\n",
    "    img2=warp_im(img2, M, img1.shape)\n",
    "    cv2.imshow(\"Morphed Face\", np.uint8(img2))\n",
    "    cv2.waitKey(0)\n",
    "    #再次取特征点\n",
    "    points2=get_landmarks(img2);\n",
    "    print(type(points1))\n",
    "    print(type(points2))\n",
    "    #创建一个list存放morph后的特征点\n",
    "    morph_points = []\n",
    "\n",
    "    #计算alpha权重特征点获取融合特征点\n",
    "    for i in range(0, len(points1)):\n",
    "        x = ( 1 - alpha ) * points1[i][0] + alpha * points2[i][0]\n",
    "        y = ( 1 - alpha ) * points1[i][1] + alpha * points2[i][1]\n",
    "        morph_points.append((x,y))\n",
    "    #为模板面孔赋值用于生成基于图片一的图片空间\n",
    "    morph_img= np.zeros(img1.shape,dtype = img1.dtype)\n",
    "    # 从tri.txt文档中读入面部三角\n",
    "    with open(\"tri.txt\") as file :\n",
    "        for line in file :\n",
    "            x,y,z = line.split()\n",
    "            \n",
    "            x = int(x)\n",
    "            y = int(y)\n",
    "            z = int(z)\n",
    "            \n",
    "            t1 = [points1[x], points1[y], points1[z]]\n",
    "            t2 = [points2[x], points2[y], points2[z]]\n",
    "            t = [morph_points[x], morph_points[y], morph_points[z]]\n",
    "            #三角融合算法，对上一步转换后的待融合图片再次取关键点，然后与模特图的关键点一起做三角融合成新的图片\n",
    "            morphTriangle(img1, img2, morph_img, t1, t2, t, alpha)\n",
    "    dst_img=morph_img\n",
    "    \n",
    "    #再次对上一步的结果图进行取点，然后运用三角仿射将模特图片脸部轮廓、关键点变形成上一步得到的脸部关键点\n",
    "    #src_img = tran_src(src_img, src_points, morph_points, face_area)\n",
    "    # Display Result\n",
    "    #cv2.imshow(\"Morphed Face\", np.uint8(dst_img))\n",
    "    #cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == '__main__' :\n",
    "\n",
    "            \n",
    "\n",
    "    # Display Result\n",
    "    #cv2.imshow(\"Morphed Face\", np.uint8(imgMorph))\n",
    "    #cv2.waitKey(0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
